{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "* [Outline](../0_Introduction/0_introduction.ipynb)\n",
    "* [Glossary](../0_Introduction/1_glossary.ipynb)\n",
    "* [5. Imaging](5_0_introduction.ipynb)    \n",
    "    * Previous: [5.3 Gridding and Degridding for using the FFT](5_3_gridding_and_degridding.ipynb)\n",
    "    * Next: [5.5 The Break Down of the Small Angle Approximation and the W-Term](#)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import section specific modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image\n",
    "from astropy.io import fits\n",
    "import aplpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 The Dirty Image and Visibility Weightings <a id='imaging:sec:weights'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have laid out the concepts of spatial filters, sampling and point spread functions, and gridding for using the fast Fourier transforms, we are ready to connect all this topics into one to show how an image is produced from a radio interferomtric array observation. The so called 'dirty' image (the reason for this term relates to the deconvolution stage which is covered in the next chapter) from an observation is approximately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I^{\\textrm{D}}(l,m) \\approx \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} S(u,v) \\, V_{\\textrm{obs}}(u,v) \\, e^{-2\\pi i(ul+vm)} \\,du\\,dv $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $V_{\\textrm{obs}}(u,v)$ is the observated visibilities, this function is a complete measurement of the uv domain, which in reality is never measured because $S(u,v)$ is the sampling function, the main component of which is the uv coverage of the baseline tracks. This is an approximate result as we have used the small angle approximation to reduce the complete 3-d visibility equation to 2-d. The limits to this approximation and how to correct for the 'w-term' is covered in the next section.\n",
    "\n",
    "The sampling function $S$, as we have seen in the previous section on gridding, contains the uv track sampling and the gridding operator. And, as we will see later in this chapter, the sampling function will be generalized to include various 'weighting' functions. It is often useful to talk of the sampled visibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V^{\\textrm{S}} = S(u,v) \\, V_{\\textrm{obs}}(u,v) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is combination of the visibility samples from all baselines onto a single visibility-space plane. Comparing the dirty image equation to that of the 2-d Fourier transform in Section 5.1 we see that the dirty image is simply the Fourier transform of the sampled visibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I^{\\textrm{D}} = \\mathscr{F}\\{V^{\\textrm{S}}\\} = \\mathscr{F}\\{ S V_{\\textrm{obs}}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $(l,m)$ and $(u,v)$ notation has been dropped here for compactness. By Fourier theory the dirty image can be interpreted as a convolution of two functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I^{\\textrm{D}} = \\mathscr{F}\\{ S \\} \\circ \\mathscr{F}\\{ V_{\\textrm{obs}}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in Section 5.2 the first function $\\mathscr{F}\\{ S \\}$ is the PSF response of the array, which we have seen in Section 5.2 is dependent on the array configuration, sky poisiton, observation time, and frequency coverage. The second function $\\mathscr{F}\\{ V_{\\textrm{obs}}\\}$ is the Fourier transform of the fully sampled visibility space. We never has access to $V_{\\textrm{obs}}$, but as we will see in the next chapter, we include *a priori* information to make an approximate reconstruction of $V_{\\textrm{obs}}$. We can think of $\\mathscr{F}\\{ V_{\\textrm{obs}}\\}$ being the ideal image $I^{\\textrm{ideal}}$ reconstructed from fulling sampling the visibility plane. Re-writing the equation above the dirty image is the convolution of the array PSF with the ideal image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I^{\\textrm{D}} = \\textrm{PSF} \\circ I^{\\textrm{ideal}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to examples from earlier in the chapter we can construct a dirty image from the original duck image and the KAT-7 PSF response from one of the observations in Section 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duck = mpimg.imread('figures/Anas_platyrhynchos_male_female_quadrat_512.png')\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b #standard grayscale conversion\n",
    "    return gray\n",
    "\n",
    "gDuck = rgb2gray(duck)\n",
    "fftDuck = np.fft.fft2(gDuck) #take the Fourier transform of the image to poduce the observed visibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdulist = fits.open('figures/psfs/KAT-7_6h60s_dec-30_10MHz_100chans-psf.fits')\n",
    "psf = hdulist[0].data #shape [polarization, channel, m, l]\n",
    "sampFunc = np.fft.fft2(psf[0,0]) #take the Fourier transform of the PSF to generate the gridded sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16,8))\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fftDirtyImg = sampFunc * fftDuck #multiplication in Fourier space is equivalent to convolution in image space\n",
    "dirtyImg = np.abs(np.fft.fftshift(np.fft.ifft2(fftDirtyImg))) #create the dirty image\n",
    "plt.title('Duck (Dirty Image)')\n",
    "imgPlot0 = plt.imshow(dirtyImg)\n",
    "imgPlot0.set_cmap('gray')\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.title('Duck')\n",
    "imgPlot1 = plt.imshow(gDuck)\n",
    "imgPlot1.set_cmap('gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: dirty image of a duck (left) created by sampling the original duck image (right) with a KAT-7 observation using 100 frequency channels, over a 6 hour observation at declination -30 degrees.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to make out the duck in the resulting dirty image. The head and long wing can be picked out, but that is likely due to the fact that we know what the original image looked like. Had we not known what the original image was we would have a difficult time deciding what we are seeing is the real image and what is due to the PSF sidelobes. This is the main issue in interferometric imaging, and perpetuated the development of deconvolution algorithms.\n",
    "\n",
    "Plotting the Fourier transform of the PSF should result in a similar image of the uv coverage plot seen in Section 5.2. Indeed it does, but notice that the color bar suggests a range of pixel values. This comes back to the overlapping uv tracks we saw in the uv coverage plots and the type of weighting scheme which was used to produce the PSF. We have been hiding a few details up until this point to simplify the presentation of sampling functions and how they affect resulting images. But, now we will generalize sampling function to introduce weighting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.title('KAT-7 Sampling Function')\n",
    "imgPlot = plt.imshow(np.abs(np.fft.fftshift(sampFunc)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: KAT-7 sampling function for an observation using 100 frequency channels, over a 6 hour observation at declination -30 degrees.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Weighting Functions, a generalization of sampling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling function, as seen in the Section 5.2 KAT-7 examples, are made up of the various baseline tracks which leads to holes and overlapping sampling. So far we have discussed sampling functions as if they are binary functions, a visibility domain pixel is given a value of 1 if a baseline track measured that location otherwise the pixel was given a value of 0. The allocation of 0 to each pixel which was not measured should make sense, as there is no information, and thuse that pixel can potentially have any value. But, for the measured pixels the sampling function value does not necessarily need to be 1. For example, say an observation results in 1000 uv positions being measured, and 100 of these positions were measured 10 times while the other 900 positions were only measured once. The 100 positions measured 10 times, will have a $\\sqrt{10}$ improvement in sensitivity compared to the other 900 positions, so perhaps those 100 spatial frequency modes should be given more weight in the resulting image because we are more sure about the accuracy of the measurement. Or, perhaps an array is made up of many different types of telescopes which leads to some baselines having a higher sensitivity to other baselines, such as in VLBI. Or, perhaps the science case is only interested in certain spatial frequencies, such as seperating point sources from large-scale gas structure. These are all examples of times when we would like to adjust our sampling function, which leads to a number of different options we need to understand when creating an image. In fact, there is no standard set of options to create an image from visibilities, a weighting function must always be decided upon before hand. The most common weighting function we must decide on when imaging is the density weighting function, but we will also discuss the beam taper and reliability weighting.\n",
    "\n",
    "As a simplification we have been discussing the PSF as being a Fourier pair of the visibility sampling function, but in fact we need to generalize to say that the PSF is a Fourier pair of a weighting function $W$, where $W$ includes the sampling function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{PSF}(l,m) \\rightleftharpoons W(u,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding $W$ we see that it is made up over a number of functions. For this section we will use the notation based on Lecture 7 of <cite data-cite='taylor1999synthesis'>Synthesis Imaging in Radio Astronomy II</cite> [CITE](http://adsabs.harvard.edu/abs/1999sira.conf.....T). For compactness the $(u,v)$ terms have been dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W = R \\cdot T \\cdot D \\cdot S $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ is a product of the the sampling function $S$ which we have all ready discussed, a density weight function $D$, an optional taper weight function $T$, and a baseline-dependent reliability weight function $R$.\n",
    "\n",
    "As a simple example let us look at some 1-d weight functions and the resulting PSFs. For this example lets say we sample the inner centre of a 1-d function. This results in a square or boxcar sampling, as seen in blue in the left-hand figure below. But, say we think some samples are 'better' than others. For example, our detector has a linear response based on the distance from the zero point. Then, we would like to weigh the samples accordingly, giving more weight to samples near the centre and less to the samples on the far edges. This results in a triangle weight function, as seen in green in the left-hand figure below. Or, perhaps a Gaussian weighting function, red in the left-hand figure below, is more appropriate. There are many types of potential weighting functions which all cover the same extent of the sample space. But, these weighting functions result in different PSF responses as seen in the right-hand figure. Using a boxcar weighting function (blue) results in the highest PSF primary lobe, but the sidelobes are significantly higher compared to other weighting functions. This relates to the idea that a smooth function in one domain is alos a smooth function in the Fourier domain, and a boxcar function is not a smooth function. A Gaussian function is a smooth function, but the sampling extent (or window in signal processing terms) is finite, this means that there is a cut off point. The PSF response of a truncated Gaussian (green) has a wider primary lobe but lower sidelobes compared to the boxcar function. As does the triangle function which can as a simple approximation to the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "whw = 48 #window half width\n",
    "\n",
    "#Square/Boxcar weight function\n",
    "squareWeights = np.zeros(512, dtype='float')\n",
    "squareWeights[256 - whw:256 + whw] = 1.\n",
    "\n",
    "#Triangle weight function\n",
    "triWeights = np.zeros(512, dtype='float')\n",
    "triWeights[256 - whw : 256] = np.linspace(0., 1., num=whw)\n",
    "triWeights[256 : 256 + whw] = np.linspace(1., 0., num=whw)\n",
    "\n",
    "#Gaussian weight function\n",
    "gaussWeights = signal.gaussian(512, std=whw / (2. * np.sqrt(2. * np.log(2.))))\n",
    "truncGaussWeights = gaussWeights * squareWeights #truncate to window width\n",
    "\n",
    "xVals = np.arange(-256,256)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16,8))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "plt.title('Weight Functions')\n",
    "plt.ylabel('Weight')\n",
    "plt.plot(xVals, squareWeights, 'b-', alpha=0.7)\n",
    "plt.plot(xVals, triWeights, 'g-', alpha=0.7)\n",
    "plt.plot(xVals, truncGaussWeights, 'r-', alpha=0.7)\n",
    "#plt.plot(xVals, gaussWeights, 'k-', alpha=0.7)\n",
    "\n",
    "ax1 = plt.subplot(2,2,2)\n",
    "plt.title('PSF (dBs)')\n",
    "plt.ylabel('Response')\n",
    "plt.plot(np.abs(np.fft.fftshift(np.fft.fft(squareWeights))), 'b-', alpha=0.7)\n",
    "plt.plot(np.abs(np.fft.fftshift(np.fft.fft(triWeights))), 'g-', alpha=0.7)\n",
    "plt.plot(np.abs(np.fft.fftshift(np.fft.fft(truncGaussWeights))), 'r-', alpha=0.7)\n",
    "#plt.plot(np.abs(np.fft.fftshift(np.fft.fft(gaussWeights))), 'k-', alpha=0.7)\n",
    "ax1.set_yscale('log')\n",
    "plt.xlim(0, 512)\n",
    "\n",
    "ax1 = plt.subplot(2,2,3)\n",
    "plt.title('Weight Functions')\n",
    "plt.ylabel('Weight')\n",
    "plt.plot(xVals, gaussWeights, 'k-', alpha=0.7)\n",
    "plt.plot(xVals, truncGaussWeights, 'r-', alpha=1.0)\n",
    "\n",
    "ax1 = plt.subplot(2,2,4)\n",
    "plt.title('PSF (dBs)')\n",
    "plt.ylabel('Response')\n",
    "plt.plot(np.abs(np.fft.fftshift(np.fft.fft(gaussWeights))), 'k-', alpha=0.7)\n",
    "plt.plot(np.abs(np.fft.fftshift(np.fft.fft(truncGaussWeights))), 'r-', alpha=0.7)\n",
    "ax1.set_yscale('log')\n",
    "plt.xlim(0, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Top: examples of 1-d weight functions: square (blue), triangle (green), truncated Gaussian (red) and the resulting PSF responses.*\n",
    "\n",
    "*Bottom: Gaussian (black) and truncated Gaussian (red) weighting function and the resulting PSF responses. The low level 'noise' in the Gaussian response is due to computational accuracy limits.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows just three possible weighting functions and the resulting PSF responses. There is no 'correct' function, each has advantages and disadvantages. A boxcar function maximizes the PSF primary lobe response but at the expense of high sidelobes. A triangle function minimizes sidelobes but at the expense of some loss in signal, and a loss of resolution (as does the Gaussian). This loss in resolution (that is a wider primary lobe) is because the weight function gives more value to the samples near the centre point, i.e. the lower spatial modes.\n",
    "\n",
    "As we will see, we can decide on a weighting function when synthesising an image to emphasize for resolution or sensitivity or a combination of the two in the density weighting funciton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Density Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When discussion of weighting functions (or schemes) comes up in conversations, presentations, or papers it is almost always referring to *density weighting functions*. A density weighting function is used to set the weights of individual uv positions based on the number of times that position is sampled in an observation. One will often see terms such as 'uniform' and 'natural' weighting which are types of density weighting functions.\n",
    "\n",
    "Going back to the KAT-7 sampling function plot shown earlier, we see that the pixels range in values. That is, during the observation those pixels in the visibility domain were sampled multiple times due to a combination of the frequency bandwidth coverage and the uv track positions. In this case, when we formed the PSF response we decided to give each sample a equal weighting, this resulted in the pixels have a range of weights. This is similar to the Gaussian weighting function in the 1-d example above. This weighting scheme is called *natural weighting* (or inverse variance weighting), it has the advantage of maximizing sensitivity but at the cost of resolution. It is often stated in literature that natural weighting lead to broad 'wings' in the PSF primary lobe, this is due to the over representation of the low spatial frequency modes.\n",
    "\n",
    "A complimentary weight scheme is *uniform weighting* (or unity weighting) which weights each sample positon equally, not matter how many times that position was sampled. This is akin to the boxcar weighting function in the 1-d example above, and results in a higher resolution image at the cost of higher PSF sidelobes.\n",
    "\n",
    "As covered in the gridding section, when gridding the sampled visibilities are placed into pixels, that is a 2-d grid array. If no samples are placed in a pixel then that pixel obviously is given zero weight. For a given pixel, if only one sample is placed at that position then it would seem we could justify giving that pixel a weight of one. But, as we have seen, multiple samples, or 'portion' of sample can be placed in a single pixel. If a sample is split between multiple pixels the total allocated weight of the combined positions for that sample should equal one, e.g. if a pixel is split between two pixels $(p_a, p_b)$, with 60% of the flux belonging in $p_a$ and 40% in $p_b$ then the sample is given a weight of 0.6 in $p_a$ and 0.4 in $p_b$. For multiple samples, the weight is the sum of the number of samples. Thus, we define a function $N_s(u,v)$ which returns the number of samples for a given pixel $(u,v)$. A natural weighting function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ D_{\\textrm{natural}}(u,v) = N_s(u,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a uniform weighting function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ D_{\\textrm{uniform}}(u,v \\, | \\, t) = \n",
    "\\begin{cases} \n",
    "    \\hfill 1    \\hfill & : N_s(u,v) > t  \\\\\n",
    "    \\hfill 0 \\hfill & : N_s(u,v) \\leq t \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $t$ is a non-negative threshold parameter, usually $t = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use a specific weighting scheme depends on the scientific aim of the observation. If you are trying to maximize resolution and are willing to lose some sensitivty then you use uniform weighting. And, if you are observing a weak source and are willing to lose some resolution then you use natural weighting. In the figures below a dirty image is show for natural and uniform weighting based on the same sky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "gc1 = aplpy.FITSFigure('figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_natural-psf.fits', figure=fig, subplot=[0.1,0.1,0.35,0.8])\n",
    "gc1.show_colorscale(vmin=-0.12, vmax=0.4, cmap='viridis')\n",
    "gc1.hide_axis_labels()\n",
    "gc1.hide_tick_labels()\n",
    "plt.title('PSF (Natural Weighting)')\n",
    "gc1.add_colorbar()\n",
    "\n",
    "gc2 = aplpy.FITSFigure('figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_uniform-psf.fits', figure=fig, subplot=[0.5,0.1,0.35,0.8])\n",
    "gc2.show_colorscale(vmin=-0.12, vmax=0.4, cmap='viridis')\n",
    "gc2.hide_axis_labels()\n",
    "gc2.hide_tick_labels()\n",
    "plt.title('PSF (Uniform Weighting)')\n",
    "gc2.add_colorbar()\n",
    "\n",
    "fig.canvas.draw()\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "gc1 = aplpy.FITSFigure('figures/dirty/KAT-7_6h60s_dec-30_10MHz_10chans_natural-dirty.fits', figure=fig, subplot=[0.1,0.1,0.35,0.8])\n",
    "gc1.show_colorscale(vmin=-2.5, vmax=5., cmap='viridis')\n",
    "gc1.hide_axis_labels()\n",
    "gc1.hide_tick_labels()\n",
    "plt.title('Dirty Image (Natural Weighting)')\n",
    "gc1.add_colorbar()\n",
    "\n",
    "gc2 = aplpy.FITSFigure('figures/dirty/KAT-7_6h60s_dec-30_10MHz_10chans_uniform-dirty.fits', figure=fig, subplot=[0.5,0.1,0.35,0.8])\n",
    "gc2.show_colorscale(-2.5, vmax=5., cmap='viridis')\n",
    "gc2.hide_axis_labels()\n",
    "gc2.hide_tick_labels()\n",
    "plt.title('Dirty Image (Uniform Weighting)')\n",
    "gc2.add_colorbar()\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Top: KAT-7 PSF using natural weighting (left) and uniform weighting (right).*\n",
    "\n",
    "*Bottom: dirty image of a simulated field using natural weighting (left) and uniform weighting (right).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source in the naturally-weighted image appear to be larger, due to the larger PSF response, compared to the uniformly-weighted image. The uniformly-weighted image has noticable 'ringing' around sources due to the sinc-like nature of the uniformly-weighted PSF response. This results in more negative PSF sidelobes compared to the smoother naturally-weighted PSF response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of an adjustable parameter-based density weighting function was introduced Daniel Briggs' thesis [<cite data-cite='briggs1995high'>High Fidelity Deconvolution of Moderately Resolved Sources</cite> &#10548;](http://www.aoc.nrao.edu/dissertations/dbriggs/). The fundamental idea presented in this thesis was that natural and uniform weighting were simply two schemes on a continuum of possible weighting schemes controlled by a single parameter called the robustness. For this reason, this generalized weighting scheme is called robust weighting (it is also often called Briggs' weighting). The robustness parameter $R$ was developed such that at its two extreme values robust weighting approximated natural and uniform weighting and the zero point for $R$ represents an equal balance between resolution and image variance. The exact implementation of robust weighting varies between imagers, but all imagers should produce similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "psfDict = collections.OrderedDict()\n",
    "psfDict['natural'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_natural-psf.fits'\n",
    "psfDict['2.0'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r2.0-psf.fits'\n",
    "psfDict['1.0'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r1.0-psf.fits'\n",
    "psfDict['0.5'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r0.5-psf.fits'\n",
    "psfDict['0.0'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r0.0-psf.fits'\n",
    "psfDict['-0.5'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r-0.5-psf.fits'\n",
    "psfDict['-1.0'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r-1.0-psf.fits'\n",
    "psfDict['-2.0'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_r-2.0-psf.fits'\n",
    "psfDict['uniform'] = 'figures/psfs/KAT-7_6h60s_dec-30_10MHz_10chans_uniform-psf.fits'\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "for key in psfDict:\n",
    "    fh = fits.open(psfDict[key])\n",
    "    img = fh[0].data\n",
    "    psfSlice = img[0,0,int(img.shape[2]/2.)] #take central slice\n",
    "    subPsfSlice = psfSlice[int(2.*psfSlice.shape[0]/5.):int(3.*psfSlice.shape[0]/5.)] #take inner fifth\n",
    "    if key=='natural':\n",
    "        c = [1.0, 0.0, 0.0]\n",
    "    elif key=='uniform':\n",
    "        c = [0.0, 1.0, 0.0]\n",
    "    else:\n",
    "        norm = (float(key) + 2.)/4.\n",
    "        c = [norm, 1.0-norm, 0.0]\n",
    "    plt.plot(subPsfSlice, label=key, color=c)\n",
    "plt.title('PSF Cross-section')\n",
    "plt.xlim(0, subPsfSlice.shape[0]-1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Cross-section of the KAT-7 PSF for natural, uniform, and a range of robust weighting functions. Natural weighting (red) results in the smallest absolute amplitude sidelobes but at the price of a wider main lobe. Uniform weighting (green) results in the narrowest main lobe but the cost of larges sidelobes. Robust weighting allows for parameterized control to make a trade off between main lobe resolution and sidelobe levels. Robustness of -2 is approximately the same as uniform weighting, and a robustness of 2 is approximately the same at natural weighting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighting function is robust if visibility samples which are oversampled during an observation are favoured over the less well-sampled visibilities, i.e. a natural weighting. Thus, a positive robustness parameter results in a weighting similar to the natural weighting, and a negative robustness results in a weighting similar to uniform weighting.\n",
    "\n",
    "A robust weighting with $R = -2$ results in an approximately uniform weighting, and $R = 2$ results in an approximately natural weighting. The robust weighting formalization also allows for $|R| > 2$. An $R > 2$ overweights the most oversampled visibility (from the shorter baselines), this will result in lower resolution than natural weighting and smoother sidelobe characteristics, this may be useful when one is interested in imaging large, diffuse structure. An $R < -2$ will up-weight the lease sampled visibilities (longest baselines) which results in a a higher resolution image than uniform weighting but also with higher sidelobe characteristics. This may be useful if one is imaging a field which has high signal to noise sources and wish to better resolve those sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform, natrual, and robust are not the only weighting schemes. There are array specific ones such as radial weighting which is used with east-west arrays. And, there is super- and sub-uniform weighting which are used to break the dependence of the PSF to the image pixel resolution and field of view. When imaging the pixel resolution and and number of pixels (field of view imaged) is adjustable. This means that the scale of the a pixel will change depending on these parameters. Thus, a low resolution image will include more visibility samples per pixel compared to a higher resolution image, and result in a different PSF response even when using the same weighting function. Super- and sub-uniform weighting can be used to set a fixed grid size for the weighting which is independent of the image resolution. This allows for PSF consistancy across different image resolutions and field of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Taper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tapers for beam scaling/spatial filter:\n",
    "\n",
    "* spatial gaussian filters, relate to sharpen and blur/smoothing filters\n",
    "* use case: smoothness at the price of resolution, filtering out lower modes to get to point sources\n",
    "\n",
    "The taper function is the same as a 2-d window function in the field of digital signal processing. A taper function acts as a 'matched filter' for a specific signal of interested. If the response for a source is known then the signal to noise can be maximized by using the same response as a taper function. In practice the most common taper function using in synthesis imaging is a 2-d Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ T(u,v\\, | \\, \\sigma) = e^{\\frac{-(u^2 + v^2)}{2\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* figure: field with different Gaussian tapers, dirty image ft back to vis, 2d convolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* relation to weighting schemes\n",
    "\n",
    "A randomly distributed array with a significant number of elements will result in an well-sampled, approximately Gaussian visibility sampling distribution. Using a uniform weighting with a taper function on the scale of the the Gaussian distribution of the visibilities will produce the same effective image as using natural weighting with no taper function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Measurment Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last component of the weighting function is the measurement reliability function $R(u,v)$. This function is intrinsic to an array and observation. If an observation is made with an array of identical elements then the relative reliability of each visibility measurement would be the same. But, say one element of an array has a higher system temperature (i.e. it is noisier) or the element has a smaller dish (less sensitive) then any baseline correlation which includes this element will result in a noisier measurment and should be weighted as such. A priori information, such as an elements effective collection area or expected system temperature, can be included in this function. Variability in the performance of individual elements during an observation is also included in this function. For example, if one of the telescopes has a noisy amplifier which needs to be fixed compared to the other telecsopes in the array. For mixed arrays such as with VLBI each individual telescope will have a different sensitivity which needs to factored in. If no relibabilty weights are included in the original observation then this function is set to unity and ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Limits of the Dirty Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion to section:\n",
    "\n",
    "* limits of dirty image, lead into deconvolution\n",
    "* figure: dense field with high sidelobes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lead into w-term issues\n",
    "* figure: w-projection effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "Next: [5.5 The Break Down of the Small Angle Approximation and the W-Term](#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
